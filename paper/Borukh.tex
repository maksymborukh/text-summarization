\documentclass[a4paper, 14pt]{extarticle}

\usepackage[utf8]{inputenc}
\usepackage[english,ukrainian]{babel}
\usepackage[T1, T2A]{fontenc}
\usepackage{dsfont}
\usepackage[export]{adjustbox}

\usepackage{amssymb, amsmath}
\usepackage{amsfonts}
\usepackage{float}
\restylefloat{table}
\usepackage{multirow}

\usepackage{fontspec}
\setmainfont{Times New Roman}

\usepackage[small]{titlesec}

\usepackage{epsfig}
\usepackage[hyphens]{url}
\usepackage{hyperref}
\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}

\usepackage[includeheadfoot, headheight=16pt]{geometry} 
\geometry{left=2.5cm}
\geometry{right=1cm}
\geometry{bottom=2cm}
\geometry{top=2cm}
\geometry{headsep=0pt}
%\renewcommand{\baselinestretch}{1.5}


\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhf{}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\newcommand{\anonsection}[1]{\section*{#1}\addcontentsline{toc}{section}{#1}}

\titleformat{\chapter}[display]
{\normalfont\bfseries}{}{0pt}{\Large}


\begin{document}
\begin{titlepage}
	
%-----------------------------------------------------------------------------------	
	
	\newpage
	\thispagestyle{empty}
	\begin{center}
		{
			\large 
			МІНІСТЕРСТВО ОСВІТИ І НАУКИ УКРАЇНИ
			
			ЛЬВІВСЬКИЙ НАЦІОНАЛЬНИЙ УНІВЕРСИТЕТ\\
			ІМЕНІ ІВАНА ФРАНКА                            
			\vspace{1cm}          
			
			Факультет прикладної математики та інформатики                     
			
			Кафедра обчислювальної математики
			\vfill   			                       
			
			\textbf{{\LARGE Курсова робота}}\\[5mm]
			
			{\LARGE Підсумовування текстів\\
				за допомогою глибоких\\[2mm]
				нейронних мереж}
			
			\bigskip
			
		}
	\end{center}
    \vfill                                              
	
	
	\newlength{\ML}
	\settowidth{\ML}{«\underline{\hspace{0.7cm}}» \underline{\hspace{2cm}}}
	\hfill\begin{minipage}{0.55\textwidth}
		Виконав: студент 4-го курсу групи ПМп-41\\
		спеціальності\\
		\underline{\makebox[9.2cm]{113 - "Прикладна математика"\\\hfill}}
		\underline{\makebox[9.2cm]{Борух М.І.\hfill}}
	\end{minipage}%
	\bigskip
	
	\hfill\begin{minipage}{0.55\textwidth}
		Керівник\\
		\underline{\makebox[9.2cm]{доцент Музичук Ю.А.\hfill}}
	\end{minipage}%
	\bigskip
	
	\hfill\begin{minipage}{0.55\textwidth}
		Національна шкала\underline{\hspace{5cm}}\\\hfill
		Кількість балів:\underline{\hspace{1.3cm}}
			Оцінка: ECTS\underline{\hspace{1.3cm}}\\\hfill
	\end{minipage}%
	\bigskip
	\bigskip
	
	\hfill\begin{minipage}{0.65\textwidth}
		Члени комісії:\hspace{0.8cm} \underline{\hspace{2cm}}
		\hspace{0.2cm} \underline{\hspace{4.5cm}}\\
		
		\hspace{3.8cm} \underline{\hspace{2cm}}
		\hspace{0.2cm} \underline{\hspace{4.5cm}}\\
		
		\hspace{3.8cm} \underline{\hspace{2cm}}
		\hspace{0.2cm} \underline{\hspace{4.5cm}}\\
		
	\end{minipage}%
	\vfill                                                 
	
	\begin{center}                                                        
		Львів - 2021                                                              
	\end{center} 
\end{titlepage}
%-----------------------------------------------------------------------------------

	\tableofcontents
	\setcounter{page}{2}
	
%-----------------------------------------------------------------------------------

	\newpage	
	\anonsection{Вступ}
	
	\qquad У сьогоднішній час, у відкритому доступі, є безліч статей різного вмісту. Здебільшого це середні або великі за обсягом роботи. Не завжди є можливість швидко дізнатися суть написаного за браком часу або бажання. Та все ж хотілося б бути в курсі опублікованого матеріалу. Можливість переглядати скорочений або підсумований текст була б чудовою, це допомогло б зменшити час на ознайомлення з новими матеріалами, також оптимізувати роботу пошукових сайтів, даючи можливість видавати більш точні результати. Все це має вплив на швидкість опрацювання інформації, можливість відділяти потрібне від другорядного. Враховуючи як стрімко наповнюється мережа новою інформацією, інструмент стискання тесту вже не є беззмістовною іграшкою.
	
	\noindent \qquad На сьогодні, нейронні мережі активно використовуюся для розв'язання таких задач. Задачу підсумовування тексту можна ділити на вибіркове та абстрактне підсумовування. Вибіркове -- вибирає важливі на думку мережі речення і повністю або частково копіює їх. Абстрактне -- відновлює основну ідею тексту в коротшій формі. Абстрактне підсумовування вважається складнішим і більш наближеним до підсумку зробленого людиною. У даній роботі буде побудовано модель, яка вміє абстрактно підсумовувати тест, а також розглянуто кроки для успішної реалізації задуманого.
	
%-----------------------------------------------------------------------------------
	 
	\newpage	
	\section{Постановка задачі}
	
	\noindent \qquad Задачу підсумовування тексту можна описати так:
	\begin{itemize}
		\item $x_{i}$ -- текст для підсумовування, $i = 1,..., n$, в процесі будуть перетворені у вектори розміру m, де число m буде залежати від довжини вхідного тексту. В результаті отримаємо матрицю $X^{n\times m} $.
		\item $y_{i}$ -- підсумок зроблений людиною, $i = 1,..., n$, в процесі перетворюються у вектори розміру k. Число k -- довжина вихідного тексту. Результат -- матриця $Y^{n\times k} $.
		\item $n$ -- кількість даних для тренування мережі.
	\end{itemize}

	\noindent \qquad Метою тренування мережі є вивчення зв'язків між $X$ та $Y$, так щоб модель максимізувала ймовірність $Y$, з урахуванням $X$:
	$$
	\max p(Y \mid X)=\max \prod_{t=1}^{n} p\left(y_{t} \mid y_{<t}, X\right),
	$$
	де $y_{<t}$ -- основні ознаки попередніх кроків.
	Класичним вибором наближення є:
	$$
	\hat{y}_{t}=\underset{y_{t}}{\arg \max } p\left(y_{t} \mid \hat{y}_{<t}, X\right),
	$$
	де $\hat{y}_{t}$ -- наближення в час $t$. Застосовується функція $\text{argmax}$ для вибору слова, цю функцію можна замінити на інші, детальніше в розділі 6. 

	\noindent \qquad Готова модель приймає текст, і повертає коротший текст, який є подібним або однаковим за ідеєю до попереднього.
	
	\noindent  \qquad Мета даної роботи дослідити та побудувати модель, що вміє підсумовувати тексти. Можна виділити такі етапи роботи:
	\begin{enumerate}
		\item Аналіз та обробка вхідних даних
		\item Реалізація глибокої нейронної мережі
		\item Побудова сумаризатора на основі даної мережі
		\item Оцінка отриманих результатів
	\end{enumerate} 

		
%--------------------------------------------------------------------------------------
		
	\newpage

	\section{Вхідні дані та їх обробка}
	\noindent \qquad Важливим етапом у побудуванні будь-якої нейронної мережі є підготовка вхідних даних. Оскільки майбутня мережа працюватиме з текстом проведемо такі основні етапи:
	
	\subsection{Обробка тексту}
	\begin{enumerate}
		\item Токенізація або лексичний аналіз
		\item Нормалізація
		\item Видалення 'шуму'
	\end{enumerate}

	1. Розбиття тексту на токени. Токеном можуть виступати як речення, так і слова. Використовуватимемо слова. Тобто після застосування даного пункту вхідний текст буде розбитий на окремі слова.
	
	2. Приведення тексту до загального шаблону. Слова в реченні мають різну форму: множина, однина, рід, закінчення слова в залежності від контексту та інші особливості вибраної мови. Використовуючи такий текст не можна, тому все зайве треба видалити, а слова звести до їх канонічної форми.
	Звести до канонічної форми можна декількома способами:
	\begin{itemize}
	 	\item Stemming - відсічення закінчення
	 	\item Lemmatization - знаходження канонічної форми
	 	\item Lemmatization з POS - канонічна форма слова, в залежності яким членом речення виступає слово
	\end{itemize}

	\noindent \qquad Найкращим варіантом є Lemmatization з POS, але така обробка вимагає багато часу, тому Stemming теж хороший варіант, оскільки потребує менше часу. Хоча для нас слова після відсічення закінчення здаватимуться незрозумілими, та для мережі це буде нормально.
	
	3. Все що не увійшло у пункт ''нормалізація'' може бути оброблено тут.А саме html теги, розмітка, метадані.
	
	\subsection{Відбір ознак}
	\noindent \qquad У попередній роботі розглядалися такі методи відбору ознак як: Count Vectorizer, TF-IDF. Цього разу використаємо звичайний one-hot encoding, де кожному слову відповідає певний номер. Припустимо, що маємо словник з двох слів, тоді:\\
	Вхід:["Hello world"]\\
	Вихід:[1 2]\\\\
	Вхід:["Hello"]\\
	Вихід:[1]\\
	
	\noindent \qquad Однак для роботи нейронної мережі, не підходить використання різних довжин векторів, тому треба звести вектор до єдиної довжини. Реалізувати це можна просто додавши 0-i до кінця або до початку тексту. Вибір було зроблено на користь першого варіанту. Нулі додаються у тому випадку, якщо поточний текст є коротшим за наперед визначену максимально допустиму довжину. Ця довжина не обов'язково має бути максимальною довжиною існуючого тексту. Дуже часно вона коротша. Викликано це тим, що розподіл довжин тексту є нерівномірним. Є 5\% тексту, довжина яких перевищує 2000 символів, а довжини менші за 500 символів складаються 50-60\%. В такому випадку не доцільно брати максимум по найдовшому тексті. Якщо текст задовгий, то все що більше за допустимий поріг відсікається.  Щоб нейронна мережа не вивчала ніякої інформації з нулювих закінчень, можна створити маску, де нулі не враховуватимуться при тренуванні мережі. Тоді:\\
	Вхід:["Hello"]\\
	Вихід:[1, 0]\\
	
	\noindent \qquad Наступне, що треба зробити -- це перетворити такі вектори так, щоб однакові за змістом слова мали однакове представлення. Такий процес називається "learn words embeddings". Є такі підходи реалізації даного процесу:\\
	
	\noindent \qquad \textbf{The Continuous Bag of Words (CBOW) Model} - неперервний мішок слів. Ідея полягає в тому, що модель намагається передбачити поточне слово враховуючи слова, які оточують бажане слово. Формуються пари слів такого вигляду (слова, які оточують слово та слово, яке хочемо  передбачити). Візьмемо таке речення: "he quick brown fox jumps over the lazy dog". Взявши розмір наволишніх  слів 2 - отримаємо такі пари:
	([quick, fox], brown), ([the, brown], quick), ([the, dog], lazy). Далі ці пари тренуються, щоб передбачати центральне слово.\\
	
	\noindent \qquad \textbf{Continuous Skip-Gram Model}. Даний підхід працює у навпаки. Моделі передається центральне слово, а вона передбачає сусідні слова.
	
	Особливості Skip-Gram:
	\begin{itemize}
		\item Добре працює на малих даних
		\item Краще визначає рідковживані слова
	\end{itemize}
	
	\newpage
	Особливості CBOW:
	\begin{itemize}
		\item Добре працює з великими даними
		\item Краще визначає частовживані слова
	\end{itemize}
	
	\noindent \qquad Існують вже пре натреновані "word embeddings". Популярними є \\Google Word2Vec,
	Stanford GloVe Embeddings\cite{datapreprocessing}. Використання вже пре натренованих "embeddings" є дуже помічним, коли датасет є невеликого розміру. На датасетах великого розміру відмінність пре натренаваних і власноруч тренованих "emdebbinds" є невеликою.
	
%--------------------------------------------------------------------------------------

	\newpage
	\section{Глибокі нейронні мережі}
	
	\noindent \qquad Завданням роботи є побудова глибокої нейронної мережі, яка б мала змогу підсумовувати тексти. Очевидно, що для такого типу завдання будь-яка мережа не підійде. Треба використати таку, що вміє працювати з послідовними даними. Текст можна розглядати як послідовність речень, а їх як послідовність слів. Основна ідея полягає в тому, що на вхід мережі послідовно подаються слова, і після кожного слова мережа запам'ятовує вихід, який передається далі, де комбінується з новим словом. Так, в кінці отримуємо вихід і стан, які будуть використовується у передбаченні наступного слова вже для підсумовування тексту. Отже, щоб зробити модель ''речення до речення'',  потрібен механізм запам'ятовування попередніх виходів, станів. Такою мережею є рекурентна нейронна мережа так, як кожен раз вона передає сама в себе попередні стани слів. Так можна зобразити РНМ.
	
	\subsection{Рекурентна нейронна мережа}
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.8\paperwidth]{img/RNN_unrolled.pdf}
		\caption{Розгорнута РНМ \cite{LSTM_pic}}
	\end{figure}
	Рекурентна нейронна мережа всередині працює наступним чином:
	$$
	h_{t}=f_{W}\left(h_{t-1}, x_{t}\right)
	$$
	На вхід подаємо $x_t$ -- вектор зі слів, $h_{t-1}$ -- стан з попереднього кроку, при $t=0$ $h_{-1}$ -- вектор нулів,  $f_W$ -- деяка активаційна функція, найчастіше сигмоїда.
	
	$$
	h_{t}=\tanh \left(W_{h h} h_{t-1}+W_{x h} x_{t}\right)
	$$
	
	$$
	y_{t}=W_{h y} h_{t}
	$$ 	
 	$W_{hh}, W_{xh}, W_{hy}$ -- матриці ваг.
 	
 	\noindent \qquad Однак, у РНМ є вагомий недолік, відомий як зникаючий градієнт. Мережа при зворотному ході оновлює параметри використовуючи алгоритм градієнтного спуску і стається так, що градієнт зменшується поки не стає константою. У такому разі модель не має можливості покращуватися і як наслідок нічого не навчається. Отримати хороший результат у такому випадку не можливо. Таке часто трапляється, коли мережа має вивчити довготривалі залежності між словами. Вирішити дану проблему можна шляхом наперед визначення ваг, але не завжди це дає бажаний результат. Тому доцільно розглянути інший тип мережі.
 	
 	\subsection{Long Short-Term Memory}
 	
 	\noindent \qquad \textbf{LSTM} -- довго короткотривала пам'ять. Така мережа здатна запам'ятовувати інформацію на довгий проміжок часу. Дана мережа складається з LSTM клітин, які в свою чергу з вхідних, вихідних воріт, а також воріт забуття. \\
 	Ворота забуття або forget gate використовуються для визначення інформації, яку потрібно забути або зберегти. 
 	$$
 	f_{t}=\sigma\left(W_{f} \cdot\left[h_{t-1}, x_{t}\right]+b_{f}\right)
 	$$
 	\noindent \qquad Цей шар на вхід отримує попередній стан $h_{t-1}$ і якусь інформацію в певний час $t$, у цьому випадку -- це слово. $W_f, b_f$ -- матриця ваг та вектор зсуву відповідно. Вхідна інформація множиться на матрицю ваг і до цього додається похибка. Після цього до отриманого результату застосовується функція $\sigma$ - сигмоїда. 
 	$$
 	\sigma = \frac{1}{1+e^{-x}}
 	$$
 	\noindent \qquad Результат після сигмоїди буде від 0 до 1, де 0 -- повністю забути, 1 -- навпаки, запам'ятати.\\
 	Наступні -- вхідні ворота (input gate) існують для визначення, якої нової інформації бракує в LSTM клітині. 
 	$$
 	i_{t}=\sigma\left(W_{i} \cdot\left[h_{t-1}, x_{t}\right]+b_{i}\right)
 	$$
 	$$
 	\tilde{C}_{t}=\tanh \left(W_{C} \cdot\left[h_{t-1}, x_{t}\right]+b_{C}\right)
 	$$
 	\noindent \qquad У цьому шарів, як і в попередньому, функція сигмоїда $\sigma$ слугує для визначення які значення треба оновити. Тобто, щось треба точно забути, зберегти або просто зменшити вплив даного параметра на майбутнє передбачення. Функція тангенс гіперболічний $\tanh$ використовується для створення нового кандидату, який повинен бути доданий до стану клітини.
 	$$
 	\tanh = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}
 	$$
 	\noindent \qquad Маючи результати з попередніх двох шарів, а саме $f_t, i_t, \tilde{C}_{t}$, можна остаточно вирішити, яку інформацію потрібно передати у наступну LSTM клітину. Записати це можна наступною формулою.
 	$$
 	C_{t}=f_{t} * C_{t-1}+i_{t} * \tilde{C}_{t}
 	$$
 	\noindent \qquad Попередній стан множиться на $f_t$ -- те, що варто забути, додається $ i_{t} * \tilde{C}_{t}$ -- новий кандидат помножений на оновленні значення попереднього. Це і буде остаточний вихідний $C_t$ стан LSTM клітини. Інший вихід буде обчислюватися в такому шарі.\\
 	\noindent \qquad Вихідні ворота -- використовуються для обчислення результатів. Маємо такі дві формули.
 	$$
 	o_{t}=\sigma\left(W_{o}\left[h_{t-1}, x_{t}\right]+b_{o}\right) 
 	$$
 	$$
 	h_{t}=o_{t} * \tanh \left(C_{t}\right)
 	$$
 	\noindent \qquad Використовуючи  сигмоїду, буде вирішено, яка попередня інформація буде присутня у новій. Далі множенням поточного стану $C_t$ після застосування гіперболічного тангенсу на $o_t$ виводяться ті частини потрібної інформації, які були вирішені у попередніх шарах\cite{LSTM_pic}. Подивимося на рисунок LSTM клітини.\\
 	\begin{figure}[H]
 		\centering
 		\includegraphics[width=0.6\paperwidth]{img/LSTM cell.pdf}
 		\caption{LSTM клітина \cite{LSTM_pic}}
 	\end{figure}
 	\noindent \qquad Також існують інші модифікація LSTM клітини. У кожної з них по-різному відбувається запам'ятовування потрібної інформації. 	
 	
 	\newpage
 	
 	\noindent \qquad Для чого потрібний саме такий механізм пам'яті й чому він добре підходить для задачі підсумовування тексту? Даний механізм добре працює з часовими рядами, тобто коли потрібно прийняти рішення використовуючи вже наявну попередню інформацію. Як відомо, текст складається з набору слів. Цей набір слів є не просто випадково згенерованим. Кожне слово якось пов'язане з попереднім або з кількома попередніми словами. Також, можна сказати, що поточне слово матиме вплив на наступне. Отже, є залежність між словами. Проявляються така залежність у кожній мові по-різному. Якщо взяти українську мову, то від займенника залежатиме закінчення наступного слова. ''Дівчина розумна'', ''Хлопець розумний'', ''Діти розумні''. Друге слово у кожному прикладі має однакове значення, та різне закінчення, бо різні займенник: вона, він, вони. В англійській мові відмінювання слів немає, але це не змінює той факт, що слова теж мають залежність один з одним. І це поєднує всі мови. Тому, щоб будувати граматично правильні речення потрібно мати механізм пам'яті. Такий тип не є надто складним, адже використовуються попереднє або два три попередні слова. З такою задачею може справитися і класична рекурентна мережа. Проте такі речення є простими, а для підсумовування тексту потрібно якомога стисліше передати зміст, тому може виникнути потреба у складніших зв'язках між словами. Для прикладу треба запам'ятати слово з речення, і те слово потрібно буде використати через декілька речень. У такому разі простої рекурентної мережі може бути не достатньо, тому доцільно застосувати більш потужний інструмент, у цьому разі це LSTM мережа. \\
 	
%--------------------------------------------------------------------------------------

	\newpage
	\section{Датасет}
	\subsection{Дані}
	\noindent \qquad Для тренування підсумовування тексту був вибраний датасет WikiHow \cite{dataset}. Розмір датасету -- 230843 записи. Містить три колонки: headline -- заголовок, title -- назва статті, text -- текст статі. Колонка title -- у даній задачі непотрібна, тому використовуватися не буде. Text -- містить інформацію відповідно до назви статті. Стаття поділена на пункти й кожний пункт має свій заголовок і відповідний текст до нього, колонка text. Headline -- всі заголовки пунктів до однієї статті. Цю колонку можна вважати підсумованим текстом. На даних з колонок ''headline'', ''text'', будемо тренувати мережу. Така мережа має підсумовувати колонку ''text'', так щоб зміст результату був схожим до колонки ''headline'' відповідного запису. Дані поділені на тренувальні, валідаційні та тестувальні у наступній пропорції 90/5/5.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\paperwidth]{img/dataset_2.pdf}
		\caption{Датасет}
	\end{figure}

	\subsection{Використання даних}
	\noindent \qquad До цих даний застосовуємо перечислені в другому пункті методи з обробки тексту. Отримуємо текст готовий до перетворення у векторизований вигляд, та спершу потрібно визначити якої довжини текст буде передано у сумаризатор, та яку довжину підсумованого тексту хочемо отримати. Для цього побудуємо графіки залежності кількості статей до їхньої довжини.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\paperwidth]{img/text_len_1.pdf}
		\caption{Довжина статей}
	\end{figure}
	\noindent \qquad Для побудови рисунку з датасету було взято 90000 записів. Статті, довжина яких менша за 150, складають 42\% від усіх даних. Заголовки статей, довжина яких менша за 60 -- 65\% від даних. Було вирішено, що модель буде тренуватися на довжині тексту в 150 слів, та підсумовуватиме текст не більше ніж у 60 слів. Брати більші довжини немає великого сенсу, оскільки на кінцевий результат це матиме незначний вплив, а на швидкість тренування моделі вливає сильно. Добавивши кілька десятків слів до вхідного тексту, час тренування може зрости на десятки хвилин. Точно сказати залежність між кількістю слів та часом тренування неможливо, через те, що на останній показник впливають і інші параметри. Якщо залишити інші параметри незмінними, а мережа підсумовуватиме текст до 60 слів і до 35 слів, то час тренування буде 123 хвилини та 86 хвилин відповідно. Також, для порівняння було натреновано модель на 180000 записів, з максимальною довжиною статей у 80 слів та підсумком до 20 слів. На тренування цієї моделі було затрачено 190 хвилин.
	
	
%--------------------------------------------------------------------------------------

	\newpage
	\section{Архітектура моделі}
	\noindent \qquad Задачею даної роботи є підсумовування текстів, тому потрібно побудувати модель, яка б надавала таку можливість. ''Послідовність до послідовності'' (seq2seq) -- підхід машинного навчання для обробки природної мови. До такого підходу відноситься підсумовування тексту. З набору слів у реченні формується нове речення, в залежності від поставленої задачі. На наступному рисунку зображений  encoder-a та decoder-a. Encoder відповідає за зчитування даних та закодовування їх у внутрішнє представлення. Decoder -- з закодованого представлення генерує слова. На рисунку зображена модель ''послідовність до послідовності''.	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\paperwidth]{img/autoencoder.pdf}
		\caption{seq2seq \cite{seq2seq_pic}}
	\end{figure}
	\noindent \qquad  На вході encoder має embedding шар після якого йде прихований LSTM шар, що дає представлення для вхідного тексту як вектор сталої довжини. Decoder, також, складається з embedding вектору для останнього згенерованого слова і LSTM шару, який з вектора сталої довжини та попереднього слова генерую наступне. Розглянемо три варіанти побудови моделі.

	\subsection{Підсумок за раз}
	\noindent \qquad Така модель генерує підсумок тексту за один раз. Недоліком такої моделі є велике навантаження на декодер, оскільки він повинен вибирати слова та їх порядок \cite{models}.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.35\paperwidth]{img/One-Shot.pdf}
		\caption{Підсумок за один раз}
	\end{figure}
	
	\subsection{Рекурсивна модель 1}
	\noindent \qquad Ця модель генерує одне слова за раз і використовує його для отримання наступного слова. Щоб сформувати наступне слово декодер використовує закодоване представлення вхідного тексту та всі попередньо сформовані слова. Підсумок створюється шляхом рекурсивного виклику моделі із попереднім передбаченим словом. Як початок підсумовування використовуються токени початку. Таким токеном може бути будь-яке слово, яке не використовується у словнику. Наприклад: <SOS>, <BOS>. Варіантів є багато і який з них вибрати не відіграє ніякої ролі. Також треба обрати такий самий токен, але для закінчення тексту, щоб мережа знала, що треба завершити підсумовування. Також підсумовування завершується у випадку досягнення максимальної наперед заданої довжини підсумку.
	
	\noindent \qquad Цей метод кращий, оскільки декодер використовує раніше сформовані слова та вихідний документ як контекст для генерації наступного слова \cite{models}.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.35\paperwidth]{img/Model-A.pdf}
		\caption{Рекурсивна модель 1}
	\end{figure}

	\subsection{Рекурсивна модель 2}
	\noindent \qquad Цей метод генерує закодоване представлення вихідного тексту. Далі, він подається в декодер на кожному кроці згенерованої вихідної послідовності. Це дозволяє декодеру створювати стани, який будуть використані для генерації наступних слів. Як і в попередньому методі, модель викликається до для кожного слова доки не сформує максимальну довжину або передбачить кінець \cite{models}.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.35\paperwidth]{img/Model-B.pdf}
		\caption{Рекурсивна модель 2}
	\end{figure}
	
%--------------------------------------------------------------------------------------

	\newpage
	\section{Генерація підсумків}
	\noindent \qquad Для підсумовування текстів у даній роботі була вибрана друга рекурсивна модель. Після того, як модель завершила тренування, можна почати отримувати результати. Як було сказано в описі до методу, щоб зробити підсумок треба подавати по одному слову на вхід декодеру. Починається з токена початку. Далі обчислюються внутрішні стани, які будуть використанні у формуванні наступного слова та ймовірності для кожного слова зі словника. На виході застосовується активаційна функція softmax.
	$$
	\text{softmax}=\frac{e^{z_{i}}}{\sum_{j=1}^{K} e^{z_{j}}}
	$$
	\noindent \qquad Маючи ймовірності до кожного слова, потрібно вирішити яке слово обирати.
	\subsection{Жадібний алгоритм}
	\noindent \qquad Найпростішим алгоритмом вибрати слово є жадібний алгоритм. Ідея полягає в тому, щоб перебрати всі ймовірності й знайти слово з найбільшою ймовірністю і обрати його. Перевага такого алгоритму є те, що він доволі швидкий, але його використання не завжди дає оптимальний результат. 
	
	\subsection{Променевий пошук}
	\noindent \qquad Інший популярний алгоритм -- пошук променя. На відміну від жадібного алгоритму цей повертає список найбільш ймовірних речень. Працює він наступний чином. Задається $n$ -- ширина променя . Обирається $n$ перший слів з ймовірностей передбачених декодером на першому кроці. Далі для кожних $n$ слів обчислюється ймовірність другого слова враховуючи попереднє. Так формуються пари слів, які є найбільш ймовірними. Алгоритм працює поки не передбачить кінець підсумку або не досягне максимальної довжини. Якщо $n$ -- один, то тоді жадібний алгоритм. 
	\noindent \qquad Недоліком такого способу є виродження тексту. Виродження тексту -- вихідний текст незв'язний, застрягає у повторюваних циклах.
	
	\subsection{Випадковий спосіб}
	\noindent \qquad Дуже простий спосіб, ідея якого обрати випадкове слово. Очевидно, що на хороші результати сподіватися не варто.
	
	\subsection{Топ-k прикладів}
	\noindent \qquad Топ-k прикладів доволі популярний спосіб вибору наступного слова. Ідея в тому, щоб з ймовірностей передбачених мережею зрізати верхні $k$.
	
	\noindent \qquad На кожному кроці, найкращі $k$ наступних слів (токенів) обираються відносно їх ймовірностей. Тобто, дано розподіл $P\left(x \mid x_{1: i-1}\right)$, визначаємо словник з найкращих $k$ слів $V^{(k)} \subset V$, так щоб $\sum_{x \in V^{(k)}} P\left(x \mid x_{1: i-1}\right)$. Нехай $p^{\prime}=\sum_{x \in V^{(k)}} P\left(x \mid x_{1: i-1}\right)$. Розподіл масштабується за наступним правилом
	$$
	P^{\prime}\left(x \mid x_{1: i-1}\right)=\left\{\begin{array}{ll}
		P\left(x \mid x_{1: i-1}\right) / p^{\prime} & \text { якщо } x \in V^{(p)} \\
		0 & \text { інакше. }
	\end{array}\right.
	$$
	Наступне слово обирається на основі даного розподілу.
	
	\noindent \qquad Невеликим недоліком такого підходу є складність у виборі хорошого параметра $k$. Адже підібрати його потрібно так, щоб отримати результати кращі від застосування пошуку променя або жадібного алгоритму. Труднощі в оптимальному підборі параметру полягають у тому, що для різних контекстів є різна кількість хороших альтернатив вибору слова. У такому випадку є ризик генерувати загальний текст. Якщо слушних варіантів мало, а параметр $k$ великий, то є ризик обрати поганий варіант слова, як наслідок -- незв'язний текст. Було б добре, як би розмір словника змінювався в залежності від контексту \cite{word_choosing}. Тому розглянемо наступний підхід.
	
	\subsection{Метод ядер}
	\noindent \qquad Метод ядер -- стохастичний метод декодування. Ідея полягає у виборі набору токенів враховуючи розподіл ймовірностей передбачених декодером. Для розподілу $P\left(x \mid x_{1: i-1}\right)$ визначається з найкращих $p$ слів словник $V^{(p)} \subset V$ настільки малий, щоб задовольнити умову
	$$
	\sum_{x \in V^{(p)}} P\left(x \mid x_{1: i-1}\right) \geq p
	$$
	Нехай $p^{\prime}=\sum_{x \in V^{(p)}} P\left(x \mid x_{1: i-1}\right)$, тоді початковий розподіл масштабується до нового 
	$$
	P^{\prime}\left(x \mid x_{1: i-1}\right)=\left\{\begin{array}{ll}
		P\left(x \mid x_{1: i-1}\right) / p^{\prime} & \text { якщо } x \in V^{(p)} \\
		0 & \text { інакше. }
	\end{array}\right.
	$$
	Вибір слова відбувається з нового розподілу.
	
	\noindent \qquad Обирається поріг $p$, так щоб сума токенів з найбільшою ймовірністю перевищувала $p$. Очевидно, що на кожному кроці словник буде динамічно змінюватися. Такий підхід має невелику перевагу над попереднім \cite{word_choosing}.

%--------------------------------------------------------------------------------------

	\newpage
	\section{Тренування моделі}
	\noindent \qquad Як було згадано раніше, було обрано другу рекурсивну модель для підсумовування текстів. Архітектура має наступний вигляд.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\paperwidth]{img/model_plot.pdf}
		\caption{Архітектура мережі}
	\end{figure}
	\noindent \qquad На вхід подається текст довжиною не більше 150 слів. Якщо текст коротший, то до його кінця додаються нулі. Цей текст складається з чисел, де кожному числу відповідає слово у словнику. Такий вектор передається у шар Embedding, де вивчаються залежності між словами. Подібні слова за значенням мають мати близьке числове представлення. На виході з цього шару отримуємо вектор розміру (None, 150, 100). None -- кількість вхідних текстів. Не є заданою наперед оскільки тренування відбувається на одній кількості, а підсумовувати треба буде іншу кількість. 150 -- довжина тексту, 100 -- розмір пре тренованих "embeddings" з GloVe, які у процесі навчання покращуються. Далі йде LSTM шар, на вхід приймає обчислення з попереднього шару і повертає останній вихід, і два внутрішні стани. Один з розмірів є 400, це наперед задане число, а саме розмір закодованого представлення, з якого надалі відбувається підсумовування тексту. На рисунку 9 з правої сторони також є вхід, сюди в процесі тренування подається підсумований текст, а в процесі вже натренованої моделі буде передаватися слово для передбачення наступного слова. Саме тому і розмір не є наперед заданий. Далі, також, йде навчання слів. Для декодування використовується LSTM, де поєднуються два внутрішні стани вхідного тексту з виходом натренованих ''embeddings'' підсумованого тексту. І останній шар з декодера обчислює для кожного слова ймовірності. Тут 5264 -- розмір словника, зі слів якого буде формуватися підсумок.
	
	\noindent \qquad Оскільки результати для такого набору параметрів були прийнятними, але не хорошими, то було вирішено змінити розмір словника для вхідних даних до 25000 слів, а для формування підсумку до 10000 слів. Також, вхідний текст не довший за 80 слів, а підсумок за 20 слів. І час тренування було збільшено. Для такої конфігурації, результати стали набагато кращими.

	\noindent \qquad Для тренування мережі було обрано два розміри датасету. Один на 90000, другий на 180000 записів. Маємо такі графіки функції втрат. 
	\begin{figure}[H]
		\centering
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=0.3\paperwidth]{img/plot_90000.pdf}
			\caption{Для 90000 записів}
		\end{minipage}%
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=0.42\paperwidth]{img/plot_180000.pdf}
			\caption{Для 180000 записів}
		\end{minipage}
	\end{figure}
	Для задач, які працюють з текстом вимірювати точність, як для інших задач, недоцільно, тому зображені функції втрат. Синім кольором позначено втрати для тренувальних даних, оранжевим для валідаційних. На 4-7 повних проходженнях тренувальних даних крізь мережу, видно, що для валідаційних даних функція втрат починає зменшуватися дуже повільно, так що графіки перетинаються. Якщо на валідаційних даних покращень не буде або різниця між тренувальним набором і валідаційним буде швидко рости, то великий шанс перетренувати модель, тоді передбачення будуть поганими. У цьому випадку, першу модель можна тренувати ще декілька проходжень і зупинитися, друга була зупинена автоматично після припинення покращення функції втрат на валідаційних даних.
	
	\noindent \qquad Дві мережі на комп'ютері з 8 ядрами та 32 ГБ пам'яті. Для першої було здійснено 10 повних проходжень тренувальних даних крізь мережу, для другої 14.  Тренування першої мережі зайняло 123 хвилини, другої -- 190 хвилин.
	
%--------------------------------------------------------------------------------------

	\newpage
	\section{Результати}
	\noindent \qquad Результати наведені для двох мереж. Порівняння методів відбору наступного слова здійснювалося на мережі натренованій на 180000 записів.
	\subsection{Приклади підсумків}
	Маємо наступний текст:\\
	normally make comment help forgive react negative situation normal overreact problem inappropriate react aggressively situation someone make passing comment betraying personal sure express anger make negative situation get bad potentially lead violence happens react aggression aggression return say nothing sometimes confront prejudice mean react especially sense response would make great impact perpetrator fact react control response calm appropriate way express answer direct question reveals prejudice make speaker uncomfortable enough ask silent silence may also make think say without say anything\\\\
	Оригінальний підсумок:\\
	the situation make your confrontation about their actions use simple comments listen to the speakers defense try not to\\\\
	Згенеровані підсумки:\\\\
	Для топ-k прикладів:\\
	$k$=10:\\
	acknowledge the situation apologize your own situation be wary of others\\
	$k$=40\\
	admit your loved your life and others take advantage of positive selftalk\\\\
	Для методу ядер:\\
	$p$=0.25:\\
	recognize the person identify your feelings be aware of your actions\\
	$p$=0.4\\
	be honest be polite with yourself be prepared to the consequences\\\\
	Для жадібного алгоритму:\\
	avoid negative thoughts avoid negative thoughts avoid negative thoughts\\\\
	Для рандомного вибору:\\
	stay positive be mindful resist the urge to it only small assert everything\\\\\\
	Для променевого пошуку:\\
	При ширині 2:\\
	avoid negative situation avoid you are be afraid too\\
	При ширині 4:\\
	acknowledge blaming persons be prepared when youre cause cause\\\\
	Ще приклад для 180000 записів:\\
	Текст: interview local temporary job placement agency recruiter find job include education work experience skill cooking cleaning organize proof resume well ensure portrays hard worker desire work hospitality industry may also require write cover letter explain experience skill prose keep cover letter page explain skill honor success culinary field far essential education experience able get entry level job kitchen assistant look job give job training promote within ask temp agency give extra consideration job contract extend make permanent job good fit\\	
	Оригінальний підсумок: place at a temp agency complete your resume apply for jobs online in person and through your temp agency\\
	Згенерований підсумок:  apply for jobs or job jobs in your area and business area make sure your business\\\\
	Приклад для 90000 записів:\\
	Текст: situation lead hypothermia medical condition affect body ability regulate body temperature include thyroid nerve damage even arthritis someone know medical condition shall want aware sensitivity environmental extreme like cold also medication treat variety condition affect body able regulate temperature question risk factor certain medication consult physician make sure wear warm clothing cold stay dry avoid activity would make sweat much cold weather watch child carefully make sure adequately dress begin shiver outside long make sure come inside regularly warm keep emergency kit car anytime drive winter simple car malfunction put risk hypothermia keep candle match blanket food water back car case get stuck break somewhere cold take supply car one huddle together warmth careful exposure cold water water need extremely cold cause hypothermia prolong exposure even cool water bring fall cold water get soon possible water attempt swim unless close safety use energy remove clothing water since help insulate water\\
	Оригінальний підсумок: know when to seek medical help do not apply direct heat to the person avoid exposure to cold understand who is at risk for take steps to prevent risk\\\
	Згенерований підсумок:  talk to your doctor about your medications
	
	\subsection{Аналіз отриманих підсумків}
	\noindent \qquad Як видно з результатів мережа може робити якісь підсумки, інколи доволі непогані. Але є багато випадків, слова у реченні є не сильно зв'язні між собою, або якась думка, фрази повторюються декілька разів. Відбувається це через виродження тексту, і щоб зменшити такий вплив було застосовано різні алгоритми відбору слів. Якщо взяти жадібний алгоритм і променевий пошук, то вони більш схильні до феномену виродження тексту. Результати це підтверджують. Щоб покращити дану мережу можна застосувати наступні кроки:
	\begin{itemize}
		\item Тренувати на більшій кількості даних і довше
		\item Використати Bidirectional LSTM шар
		\item Додати шар ''Уваги''
		\item Будувати складнішу архітектуру моделі
	\end{itemize}
	
	\subsection{Метрика ROUGE}
	\noindent \qquad ROUGE -- Recall-Oriented Understudy for Gisting Evaluation. ROUGE -- це не одна метрика, а цілий набір. Сюди входять: ROUGE-N, Recall, Precision, F1 Score, ROUGE-L, ROUGE-S.
	
	\noindent \qquad Recall (повнота, чутливість) --  є часткою загального числа позитивних зразків, яку було дійсно знайдено. Рахується як кількість n-grams в підсумованому тексті та оригінальному підсумку та бере їх перетин, який ділить на загальну кількість n-grams в оригіналі.
	$$
	\frac{\text { count }_{\text {match }}\left(\text { gram }_{n}\right)}{\text { count }_{\text {origin }}\left(\text { gram }_{n}\right)}
	$$
	\noindent \qquad Precision (влучність) --  є часткою релевантних зразків серед знайдених. Обчислюється, так само як і попередня метрика, тільки ділиться на загальну кількість n-grams в підсумованому тексті з моделі. 
	$$
	\frac{\text { count }_{\text {match }}\left(\text { gram }_{n}\right)}{\text { count }_{\text {model }}\left(\text { gram }_{n}\right)}
	$$
	\noindent \qquad F1-Score -- показник ефективності моделі, який враховує число правильно визначених результатів, поділене на всі позитивні та правильно визначених на число всіх зразків.
	$$
	2 * \frac{\text { precision }^{*} \text { recall }}{\text { precision }+\text { recall }}
	$$
	\noindent \qquad  ROUGE-L -- вимірює найдовшу спільну підпослідовність (LCS) між вихідною моделлю та оригінальним підсумком. Підраховуємо найдовшу послідовність токенів, яку поділяють обидва підсумки: передбачений і оригінальний.
	$$
	\frac{\text { LCS(gram } \left._{n}\right)}{\text { count(gram } \left._{n}\right)}
	$$
	
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			&  & f & p & r \\
			\hline
			\multirow{3}{*}{ROUGE-1} & Avg & 0.144 & 0.19 & 0.121 \\
			\cline{2-5}
			& Max & 0.562 & 0.75 & 0.473 \\
			\cline{2-5}
			& Min & 0.0 & 0.0 & 0.0 \\
			\hline
			\multirow{3}{*}{ROUGE-L} & Avg & 0.126 & 0.176 & 0.103 \\
			\cline{2-5}
			& Max & 0.471 & 0.75 & 0.462 \\
			\cline{2-5}
			& Min & 0.0 & 0.0 & 0.0 \\
			\hline
		\end{tabular}
		\caption{ROUGE для моделі}
	\end{table}
	
	\noindent \qquad  Як видно з таблиці, середні результати є низькими. Оскільки 0 -- не знайдено жодного збігу, 1 -- речення однакові. Але враховуючи, що оцінка тексту є складною і вона базується на пошуку однакових слів у двох текстах, то можна сказати, що результати є доволі непоганими. Непогані тому, що під час підсумовування було зменшено словник з якого робилося підсумовування, тому не всі слова з оригінального тексту увійшли у підсумок. Проте, тренування здійснювалося з використанням натренованих векторів слів, де схожі за змістом слова мали близьке представлення. Звідси випливає, що підсумок зроблений моделлю може бути хорошим, оскільки передає ту саму думку, що і підсумок зроблений людиною, але іншими словами.

	
%--------------------------------------------------------------------------------------


 	\newpage
 	\anonsection{Висновки}
 	
 	
 	
%--------------------------------------------------------------------------------------

 	\newpage
 	\phantomsection
 	\addcontentsline{toc}{section}{Список літератури}
 	\renewcommand\bibname{Список літератури}
 	\bibliography{plain}
	\begin{thebibliography}{9}
		\bibitem{datapreprocessing}
			Sharmila Polamuri 
			\textit{MOST POPULAR WORD EMBEDDING TECHNIQUES IN NLP }
			\newline [Електронний ресурс] 
			/ Sharmila Polamuri // dataaspirant.com.-2020.-
			Режим доступу: https://dataaspirant.com/word-embedding-techniques-nlp/\#t-1597717516717
			
		\bibitem{LSTM_pic}
			Сhrisolah
			\textit{Understanding LSTM Networks}
			[Електронний ресурс] 
			/ Сhrisolah // colah.github.io.-2015.-
			Режим доступу: http://colah.github.io/posts/2015-08-Understanding-LSTMs/
			
		\bibitem{dataset}
			Mahnaz Koupaee, William Yang Wang 
			\textit{WikiHow: {A} Large Scale Text Summarization Dataset} \\2018. arXiv: 1810.09305 [cs.LG].
			
		\bibitem{seq2seq_pic}
			Sachin Abeywardana
			\textit{Sequence to sequence tutorial}
			[Електроний ресурс]
			/Abeywardana Sachin // towardsdatascience.com.-2017-
			Режим доступу:
			https://towardsdatascience.com/sequence-to-sequence-tutorial-4fde3ee798d8
			
		\bibitem{models}
			Jason Brownlee
			\textit{Encoder-Decoder Models for Text Summarization in Keras} 
			[Електроний ресурс]
			/Brownlee Jason // machinelearningmastery.com.-2017-
			Режим доступу:
			https://machinelearningmastery.com/encoder-decoder-models-text-summarization-keras/
			
		\bibitem{word_choosing}
			Ari Holtzman і Jan Buys і Li Du і Maxwell Forbes і Yejin Choi
			\textit{The Curious Case of Neural Text Degeneration} \\2020. arXiv: 1904.09751 [cs.CL].
			 
		
	\end{thebibliography}

%--------------------------------------------------------------------------------------

\end{document}