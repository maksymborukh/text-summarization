\documentclass[a4paper, 14pt]{extarticle}

\usepackage[utf8]{inputenc}
\usepackage[english,ukrainian]{babel}
\usepackage[T1, T2A]{fontenc}
\usepackage{dsfont}
\usepackage[export]{adjustbox}

\usepackage{amssymb, amsmath}
\usepackage{amsfonts}
\usepackage{float}
\restylefloat{table}

\usepackage{fontspec}
\setmainfont{Times New Roman}

\usepackage[small]{titlesec}

\usepackage{epsfig}
\usepackage[hyphens]{url}
\usepackage{hyperref}
\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}

\usepackage[includeheadfoot, headheight=16pt]{geometry} 
\geometry{left=2.5cm}
\geometry{right=1cm}
\geometry{bottom=2cm}
\geometry{top=2cm}
\geometry{headsep=0pt}
%\renewcommand{\baselinestretch}{1.5}


\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhf{}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\newcommand{\anonsection}[1]{\section*{#1}\addcontentsline{toc}{section}{#1}}

\titleformat{\chapter}[display]
{\normalfont\bfseries}{}{0pt}{\Large}


\begin{document}
\begin{titlepage}
	
%-----------------------------------------------------------------------------------	
	
	\newpage
	\thispagestyle{empty}
	\begin{center}
		{
			\large 
			МІНІСТЕРСТВО ОСВІТИ І НАУКИ УКРАЇНИ
			
			ЛЬВІВСЬКИЙ НАЦІОНАЛЬНИЙ УНІВЕРСИТЕТ\\
			ІМЕНІ ІВАНА ФРАНКА                            
			\vspace{1cm}          
			
			Факультет прикладної математики та інформатики                     
			
			Кафедра обчислювальної математики
			\vfill   			                       
			
			\textbf{{\LARGE Курсова робота}}\\[5mm]
			
			{\LARGE Підсумовування текстів\\
				за допомогою глибоких\\[2mm]
				нейронних мереж}
			
			\bigskip
			
		}
	\end{center}
    \vfill                                              
	
	
	\newlength{\ML}
	\settowidth{\ML}{«\underline{\hspace{0.7cm}}» \underline{\hspace{2cm}}}
	\hfill\begin{minipage}{0.55\textwidth}
		Виконав: студент 4-го курсу групи ПМп-41\\
		спеціальності\\
		\underline{\makebox[9.2cm]{113 - "Прикладна математика"\\\hfill}}
		\underline{\makebox[9.2cm]{Борух М.І.\hfill}}
	\end{minipage}%
	\bigskip
	
	\hfill\begin{minipage}{0.55\textwidth}
		Керівник\\
		\underline{\makebox[9.2cm]{доцент Музичук Ю.А.\hfill}}
	\end{minipage}%
	\bigskip
	
	\hfill\begin{minipage}{0.55\textwidth}
		Національна шкала\underline{\hspace{5cm}}\\\hfill
		Кількість балів:\underline{\hspace{1.3cm}}
			Оцінка: ECTS\underline{\hspace{1.3cm}}\\\hfill
	\end{minipage}%
	\bigskip
	\bigskip
	
	\hfill\begin{minipage}{0.65\textwidth}
		Члени комісії:\hspace{0.8cm} \underline{\hspace{2cm}}
		\hspace{0.2cm} \underline{\hspace{4.5cm}}\\
		
		\hspace{3.8cm} \underline{\hspace{2cm}}
		\hspace{0.2cm} \underline{\hspace{4.5cm}}\\
		
		\hspace{3.8cm} \underline{\hspace{2cm}}
		\hspace{0.2cm} \underline{\hspace{4.5cm}}\\
		
	\end{minipage}%
	\vfill                                                 
	
	\begin{center}                                                        
		Львів - 2021                                                              
	\end{center} 
\end{titlepage}
%-----------------------------------------------------------------------------------

	\tableofcontents
	\setcounter{page}{2}
	
%-----------------------------------------------------------------------------------

	\newpage	
	\anonsection{Вступ}
	
	\qquad У сьогоднійшній час, у відкритому доступі, є безліч статей різного вмісту. Здебільшого це середні або великі за обсягом роботи. Не завжди є можливість швидко дізнатися суть написаного за браком часу або бажання. Та все ж хотілося б бути в курсі опублікованого матеріалу. Можливість переглядати скорочений або підсумований текст була б чудовою, це допомогло б зменшити час на ознайомлення з новими матеріалами, також оптимізувати роботу пошукових сайтів, даючи можливість видавати більш точні результати. Все це має вплив на швидкість опрацювання інформації, можливість відділяти потрібне від другорядного. Враховуючи як стрімко наповнюється мережа новою інформацією, інструмент стискання тесту вже не є беззмістовною іграшкою.\\
	
	\noindent \qquad На сьогодні, нейронні мережі активно використовуюся у вирішенні таких задач. У даній роботі буде побудовано модель, яка вміє підсумовувати тест, а також розглянуто кроки для успішної реалізації задуманого.
	
%-----------------------------------------------------------------------------------
	 
	\newpage	
	\section{Постановка задачі}
	
	\qquad Задачу підсумовування тексту можна описати так:
	\begin{itemize}
		\item $x_{i}$ - вхідні дані (речення), $i = 1,..., n$, в процесі будуть перетворені у вектори розміру m, де число m буде залежати від довжини вхідного речення. В результаті отримаємо матрицю $X^{n\times m} $
		\item $y_{i}$ - вхідні дані (речення), $i = 1,..., n$, в процесі перетворюються у вектори розміру k. Число k -- довжина вихідного речення. Результат -- матриця $Y^{n\times k} $
	\end{itemize}

	\noindent \qquad Готова модель приймає текст, і повертає коротший текст, який є подібним або однаковим за ідеєю до попереднього.
	
	\noindent  \qquad Мета даної роботи дослідити та побудувати модель, що вміє підсумовувати тексти. Можна виділити такі етапи роботи:
	\begin{enumerate}
		\item Аналіз та обробка вхідних даних
		\item Реалізація глибокої нейронної мережі
		\item Побудова сумаризатора на основі даної мережі
		\item Оцінка отриманих результатів
	\end{enumerate} 

		
%--------------------------------------------------------------------------------------
		
	\newpage

	\section{Вхідні дані та їх обробка}
	Важливим етапом у побудуванні будь-якої нейронної мережі є підготовка вхідних даних. Оскільки майбутня мережа працюватиме з текстом проведемо такі основні етапи:
	
	\subsection{Обробка тексту}
	\begin{enumerate}
		\item Токенізація або лексичний аналіз
		\item Нормалізація
		\item Видалення 'шуму'
	\end{enumerate}

	1. Розбиття тексту на токени. Токененом можуть виступати як речення, так і слова. Використовуватимемо слова. Тобто після застосування даного пункту вхідний текст буде розбитий на окремі слова.
	
	2. Приведення тексту до загального шаблону. Слова в реченні мають різну форму: множина, однина, рід, закінчення слова в залежності від контексту та інші особливості вибраної мови. Використовуючи такий текст не можна, тому все зайве треба видалита, а слова звести до їх канонічної форми.
	Звести до канонічної форми можна декількома способами:
	\begin{itemize}
	 	\item Stemming - обрублення закінчення
	 	\item Lemmatization - знаходження канонічної форми
	 	\item Lemmatization з POS - канонічна форма слова, в залечності яким членом речення виступає слово
	\end{itemize}

	Найкращим варіантом є Lemmatization з POS, але така обробка вимагає багато часу, тому Stemming теж хороший варіант, так-як потребує менше часу. Хоча для нас слова після обрублення закінчення здаватимуться незрозумілими, та для мережі це буде нормально.
	
	3. Все що не увійшло у пункт "нормалізація" може бути оброблено тут. В саме html теги, розмінка, метадані.
	
	\subsection{Відбір ознак}
	\qquad У попередній роботі розглядалися такі методи відбору ознак як: Count Vectorizer, TF-IDF. Цього разу використаємо звичайний one-hot encoding, де кожному слову відповідає певний номер. Припустимо, що маємо словник з двох слів, тоді:\\
	Вхід:["Hello world"]\\
	Вихід:[1 2]\\\\
	Вхід:["Hello"]\\
	Вихід:[1]\\
	
	Однак для роботи нейроної мережі, не підходить використання різних довжин векторів, тому треба звести ветор до єдиної довжини. Реалізувати це можна просто додавши 0-i до кінця тексту. Щоб нейронна мережа не вивчала ніякої інформації з таких закінчень, можна стоворити маску, де нулі не враховуватимуться при тренуванні мережі. Тоді:\\
	Вхід:["Hello"]\\
	Вихід:[1, 0]\\
	
	Наступне, що треба зробити -- це перетворити такі вектори так, щоб однакові за змістом слова мали однакове представлення. Такий процес називається "learn words embeddings". Є такі підходи реалізації даного процесу:
	
	\textbf{The Continuous Bag of Words (CBOW) Model} - неперервний мішок слів. Ідея полягає в тому, що модель намагається передбачити поточне слово враховуючи слова, які оточують дане слово. Формуються пари слів такого вигляду (слова, які оточують слово, слово, яке хочимо передбачити). Візьмемо таке речення: "he quick brown fox jumps over the lazy dog". Взявши розмір оточуючих слів 2 - отримаємо такі пари:
	([quick, fox], brown), ([the, brown], quick), ([the, dog], lazy). Далі ці пари тренуються, щоб передбачати центральне слово.
	
	\textbf{Continuous Skip-Gram Model}. Даний підхід працює у навпаки. Моделі передається центральне слова, а вона передбачає сусідні слова.
	
	Особливості Skip-Gram:
	\begin{itemize}
		\item Добре працює на малих датасетах
		\item Краще визначає рідковживані слова
	\end{itemize}
	
	Особливості CBOW:
	\begin{itemize}
		\item Добре працює з великими датасетами
		\item Краще визначає частовживані слова
	\end{itemize}
	
	Існують вже пре-натреновані "word embeddings". Популярними є \\Google Word2Vec,
	Stanford GloVe Embeddings\cite{datapreprocessing}. Використання вже пре-натренованих "embeddings" є дуже помічним, коли датасет є невеликого розміру. На датасетах великого розміру відмінність між пре-натренаваними і власноруч тренованими "emdebbinds" є невеликою.
	
%--------------------------------------------------------------------------------------

	\newpage
	\section{Глибокі нейронні мережі}
	
	\qquad Завдання роботи є побудова глибокої нейроної мережі, яка б мала змогу підсумовувати тексти. Очевидно, що для такого типу завдання будь-яка мережа не підійде. Треба використати таку, що вміє працювати з послідовними даними. Текст можна розглядати як послідовність речень, а їх в свою чергу як послідовність слів. Основна ідея полягає в тому, що на вхід мережі послідовно даються слова, і після кожного слова мережа запам'ятовує вихід, який передається далі, де комбінується з новим словом. Так, в кінці отримуємо вихід і стан, які будуть використовується у передбаченні наступного слова вже для підсумовування тексту. Отже, щоб зробити модель "речення до речення"  потрібен механізм запам'ятовування попередніх виходів, станів. Такою мережею є рекурентна нейронна мережа так, як кожен раз вона передає сама в себе попередні стани слів. Так можна зобразити РНМ.
	
	\subsection{Рекурентна нейронна мережа}
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.8\paperwidth]{img/RNN_unrolled.pdf}
		\caption{Розгорнута РНМ \cite{LSTM_pic}}
	\end{figure}
	Рекурентна нейронна мережа всередині працює наступним чином:
	$$
	h_{t}=f_{W}\left(h_{t-1}, x_{t}\right)
	$$
	На вхід подаємо $x_t$ -- вектор із слів, $h_{t-1}$ -- стан з попереднього кроку, при $t=0$ $h_{-1}$ -- вектор нулів,  $f_W$ -- деяка активаційна функція, найчастіше сигмоїда.
	
	$$
	h_{t}=\tanh \left(W_{h h} h_{t-1}+W_{x h} x_{t}\right)
	$$
	
	$$
	y_{t}=W_{h y} h_{t}
	$$ 	
 	$W_{hh}, W_{xh}, W_{hy}$ -- матриці ваг.
 	
 	Все, здається, добре, однак, у РНМ є вагомий недолік, відомий як зникаючий градієнт. Мережа при зворотньому ході оновлює параметри використовуючи алгоритм градієнтного спуску і стається так, що градієнт зменшується поки не стає константою. У такому разі модель не має можливості покращуватися і як наслідок нічого не навчається. Отримати хороший результат у такому випадку не можливо. Таке часто трапляється, коли мережа має вивчити довготривалі залежності між словами. Вирішити дану проблему можна шляхом наперед визначення ваг, але не завжди це дає бажаний результат. Тому доцільно розглянути інший тип мережі.
 	
 	\subsection{Long Short-Term Memory}
 	
 	\qquad \textbf{LSTM} -- довго-короткотривала пам'ять. Така мережа здатна запам'ятовувати інформацію на довгий період часу. Дана мережа складається LSTM клітин, які в свою чергу з вхідних, вихідних воріт, а також воріт забуття. \\
 	Ворота забуття або forget gate використовуються для визначення інформації, яку потрібно забути або зберегти. 
 	$$
 	f_{t}=\sigma\left(W_{f} \cdot\left[h_{t-1}, x_{t}\right]+b_{f}\right)
 	$$
 	Цей шар на вхід отримує попередній стан $h_{t-1}$ і якусь інформацію в певний час $t$, у даному випадку -- це слово. $W_f, b_f$ -- матриця ваг та вектор похибки відповідно. Вхідна інформація множиться на матрицю ваг і до цього додається похибка. Після цього до отриманого результату застосовується функція $\sigma$ - сигмоїда. 
 	$$
 	\sigma = \frac{1}{1+e^{-x}}
 	$$
 	Результат після сигмоїди буде від 0 до 1, де 0 -- повністю забути, 1 -- навпаки, запам'ятати.\\
 	Наступні -- вхідні ворота (input gate) існуються для визначення, якої нової інформації бракує в LSTM клітині. 
 	$$
 	i_{t}=\sigma\left(W_{i} \cdot\left[h_{t-1}, x_{t}\right]+b_{i}\right)
 	$$
 	$$
 	\tilde{C}_{t}=\tanh \left(W_{C} \cdot\left[h_{t-1}, x_{t}\right]+b_{C}\right)
 	$$
 	У цьому шарів, як і в попередньому, функція сигмоїда $\sigma$ слугує для визначення які значення треба оновити. Тобто, щось треба точно забути, зберегти або просто зменшити вплив даного параметра на майбутнє передбачення. Функція тангенс гіперболічний $\tanh$ використовується для створення нового кандидату, який повинен бути доданий до стану клітини.
 	$$
 	\tanh = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}
 	$$
 	Маючи обрахунки з попередніх двох шарів, а саме $f_t, i_t, \tilde{C}_{t}$, можна остаточно вирішити, яку інформацію потрібно передати у наступну LSTM клітину. Записати це можна наступною формулою.
 	$$
 	C_{t}=f_{t} * C_{t-1}+i_{t} * \tilde{C}_{t}
 	$$
 	Попередній стан множиться на $f_t$ -- те, що варто забути, додається $ i_{t} * \tilde{C}_{t}$ -- новий кандидат помножений на оновленні значення попереднього. Це і буде остаточний вихідний $C_t$ стан LSTM клітини. Інший вихід буде обчислюватися в такому шарі.\\
 	Вихідні ворота -- використовуються для обчислення результатів. Маємо такі дві формули.
 	$$
 	o_{t}=\sigma\left(W_{o}\left[h_{t-1}, x_{t}\right]+b_{o}\right) 
 	$$
 	$$
 	h_{t}=o_{t} * \tanh \left(C_{t}\right)
 	$$
 	Використаючи сигмоїду, вирішиться, яка попередня інформація буде присутня у новій. Далі множенням поточного стану $C_t$ після застосування гіперболічного тангенсу на $o_t$ виводяться ті частини потрібної інформації, які були вирішені у попередніх шарах\cite{LSTM_pic}. Подивимося на рисунок LSTM клітини.\\
 	\begin{figure}[H]
 		\includegraphics[width=0.8\paperwidth]{img/LSTM cell.pdf}
 		\caption{LSTM клітина \cite{LSTM_pic}}
 	\end{figure}
 	Також існуються інші модифікація LSTM клітини. У кожної з них по-різному відбувається запам'ятовування потрібної інформації.
 	
 	\newpage
 		
 	\quad Для чого потрібний саме такий механізм пам'яті і чому він добре підходить для задачі підсумовування тексту? Даний механіз добре пряцює з часовими рядами, тобто коли потрібно прийняти рішення використовуючи вже існуючу попередню інформацію. Як відомо текст складається з набору слів. Цей набір слів є не просто випадково згенерованим. Кожне слово якось пов'язане з попереднім або з кількома попередніми словами. Також, можна сказати, що поточне слово матиме вплив на наступне. Отже, є залежність між словами. Проявляються така залежність у кожній мові по-різному. Якщо взяти українську мову, то від займенника залежатиме закінчення наступного слова. ''Дівчина розумна'', ''Хлопець розумний'', ''Діти розумні''. Друге слово у кожному прикладі має однакове значення, та різне закінчення, бо різні займенник: вона, він, вони. В англійській мові з відміннювання слів немає, але це не змінює той факт, що слова теж мають залежність одни з одним. І це поєднює всі мови. Тому, щоб будувати граматично правильні речення потрібно мати механізм пам'яті. Такий тип не є надто складним, адже використовуються попереднє або два-три попередні слова. З такою задачею може справитися і класична рекурентна мережа. Проте такі речення є простими, а для підсумовування тексту потрібно якомога стисліше передати зміст, тому може виникнути потреба у більш складних зв'язках між словами. Для прикладу треба запам'ятати слово з речення, і те слово подрібно буде використати через декілька речень. У такому разі простої рекурентної мережі може бути не достатньо, тому доцільно застосувати більш потужний інструмент, у цьому разі це LSTM мережа. 
 	
%--------------------------------------------------------------------------------------

	\newpage
	\section{Модель}
	\subsection{Датасет}
	Для тренування підсумовування тексту був вибраний датасет WikiHow \cite{dataset}. Розмір датасету -- 230,843 записи. Містить три колонки: headline -- заголовок, title -- назва статті, text -- текст статі. Колонка title -- у даній задачі непотрібна, тому використовуватися не буде. Text -- містить інформацію відповідно до назви статті. Стаття поділенна на пункти і кожний пункт має свій заголовок і відповідний текст до нього, колонка text. Headline -- всі заголовки пунктів до однієї статті. Цю колонку можна вважати підсумованим текстом. На такий даних з колонок ''headline'', ''text'', будемо тренувати мережу. Така мережа має підсумовувати колонку ''text'', так щоб зміст результату був схожим до колонки ''headline'' відповідного запису. Дані поділені на тренувальні, валідаційні та тестувальні у наступній пропорції 90/5/5.
		
	
	\subsection{Тренування моделі}
	
	
%--------------------------------------------------------------------------------------

	
	
%--------------------------------------------------------------------------------------


 	\newpage
 	\anonsection{Висновки}
 	
 	
 	
%--------------------------------------------------------------------------------------

 	\newpage
 	\phantomsection
 	\addcontentsline{toc}{section}{Список літератури}
 	\renewcommand\bibname{Список літератури}
 	\bibliography{plain}
	\begin{thebibliography}{9}
		\bibitem{datapreprocessing}
			Sharmila Polamuri 
			\textit{MOST POPULAR WORD EMBEDDING TECHNIQUES IN NLP }
			\newline [Електронний ресурс] 
			/ Sharmila Polamuri // dataaspirant.com.-2020.-
			Режим доступу: https://dataaspirant.com/word-embedding-techniques-nlp/\#t-1597717516717
			
		\bibitem{LSTM_pic}
			Сhrisolah
			\textit{Understanding LSTM Networks}
			[Електронний ресурс] 
			/ Сhrisolah // colah.github.io.-2015.-
			Режим доступу: http://colah.github.io/posts/2015-08-Understanding-LSTMs/
			
		\bibitem{dataset}
			Mahnaz Koupaee, William Yang Wang 
			\textit{WikiHow: {A} Large Scale Text Summarization Dataset} \\2018. arXiv: 1810.09305 [cs.LG].
			
		\bibitem{anchors}
			Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin
			\textit{Anchors: High-Precision Model-Agnostic Explanations}, 2018
			
		\bibitem{anchors_a}
			Christoph Molnar
			\textit{Interpretable machine learning. A Guide for Making Black Box Models Explainable} / Molnar Christoph, 2020
			
		\bibitem{alibi}
			Klaise, Janis i Van Looveren, Arnaud i Vacanti, Giovanni i Coca, Alexandru
			\textit{Alibi: Algorithms for monitoring and explaining machine learning models}[Електронний ресурс] / github.-2020.- Режим доступу: https://github.com/SeldonIO/alibi
			 
		
		
	\end{thebibliography}

%--------------------------------------------------------------------------------------

\end{document}